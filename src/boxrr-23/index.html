<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Berkeley Open Extended Reality Recording Dataset 2023 (BOXRR-23) | Berkeley RDI</title>
    <meta name="description" content="This dataset contains 4,717,215 motion capture recordings generated by 105,852 real users of extended reality (XR) devices. We sourced these recordings from a number of broadly publicly-available sources, converted them all into a single format, combined them with additional metadata from further open-access APIs, removed identifiable user data, and finally packaged the recordings into a unified dataset for use by the research community.">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha512-1sCRPdkRXhBV2PBLUdRb4tMg1w2YPf37qatUFeS7zlBy7jJI8Lf4VHwWfZZfpXtYSLy85pkm9GaYVYMfw5BC1A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link href="metaverse.css" rel="stylesheet">
  </head>
  <body>
    {% include "header" %}
    <div id="carousel" class="carousel slide" data-bs-ride="false">
      <div class="carousel-inner">
        <div class="carousel-item active">
          <div class="carousel-img" style="background-image: url('img/ident.png')"></div>
          <div class="carousel-caption d-none d-md-block">
            <h5 class="text-shadow mt-0 mb-3">Berkeley Open Extended Reality Recording Dataset 2023 (BOXRR-23)</h5>
            <p>
              <a class="btn btn-success" href="https://berkeley.qualtrics.com/jfe/form/SV_6nxKtD96YQMViu2" target="_blank"><i class="fa fa-download"></i>&nbsp; Download Data</a> &nbsp;
              <a class="btn btn-secondary" href="https://rdi.berkeley.edu/metaverse/boxrr-23/datasheets.pdf" target="_blank"><i class="fa fa-page"></i>&nbsp; Datasheets</a> &nbsp;
              <a class="btn btn-secondary" href="https://rdi.berkeley.edu/metaverse/boxrr-23/nutrition.pdf" target="_blank"><i class="fa fa-page"></i>&nbsp; Nutrition</a> &nbsp;
              <!--
              <a class="btn btn-primary" href="https://arxiv.org/abs/2302.08927" target="_blank"><i class="fa fa-file-lines"></i>&nbsp; Read Paper</a> &nbsp;
              <a class="btn btn-primary" href="https://github.com/MetaGuard/Identification" target="_blank"><i class="fab fa-github"></i>&nbsp; View Repo</a>
              -->
            </p>
          </div>
        </div>
      </div>
    </div>
    <div class="bg-dark text-white py-4">
      <div class="container text-left">
        <div class="row">
          <div class="col-2">
            <p class="mt-0 mb-0">DATA</p>
            <h4>4.71 TB</h4>
          </div>
          <div class="col-2">
            <p class="mt-0 mb-0">USERS</p>
            <h4>105,852</h4>
          </div>
          <div class="col-2">
            <p class="mt-0 mb-0">RECORDINGS</p>
            <h4>4,717,215</h4>
          </div>
          <div class="col-2">
            <p class="mt-0 mb-0">COUNTRIES</p>
            <h4>50+</h4>
          </div>
          <div class="col-2">
            <p class="mt-0 mb-0">XR DEVICES</p>
            <h4>30+</h4>
          </div>
          <div class="col-2">
            <p class="mt-0 mb-0">FORMAT</p>
            <h4>.XROR</h4>
          </div>
        </div>
      </div>
    </div>
    <div class="py-5 bg-light">
      <div class="container">
          <p class="text-secondary">2023 &nbsp;|&nbsp; Vivek Nair &middot; Wenbo Guo &middot; Rui Wang &middot; James F. O’Brien &middot; Dawn Song</p>
          <p class="text-justify">The BOXRR-23 dataset contains 4,717,215 motion capture recordings generated by 105,852 real users of extended reality (XR) devices. We sourced these recordings from a number of broadly publicly-available sources, converted them all into a single format, combined them with additional metadata from further open-access APIs, removed identifiable user data, and finally packaged the recordings into a unified dataset for use by the research community. The dataset totals 4.7 TB in compressed size, and expands to over 8.0 TB of raw data. For ease of access, we have split the data into 106 chunks, each containing up to 1,000 users, with an average size of 45 GB per chunk.</p>
        </div>
      </div>
    </div>
    <div>
		<div class="bg-secondary text-white mt-0 mb-0">
			<div class="container features py-4 mb-4 text-center text-lg-start">
				<div class="row align-items-center">
					<div class="col-lg-8">
						<p><b>As seen in:</b></p>
            <ul>
              <li>Vivek Nair, Wenbo Guo, Justus Mattern, Rui Wang, James F. O’Brien, Louis Rosenberg, and Dawn Song. "Unique Identification of 50,000+ Virtual Reality Users from Head & Hand Motion Data." arXiv, 17 February 2023. <a style="color: #ddf" href="https://doi.org/10.48550/arXiv.2302.08927">doi:10.48550/arXiv.2302.08927</a>.</li>
            </ul>
					</div>
					<div class="col-lg-4">
						<p class="mt-5 mt-lg-0"><b>We appreciate the support of:</b></p>
						<a href="https://rdi.berkeley.edu/" target="_blank">
							<img src="img/rdi.jpg" height="70" style="border-radius: 50%; margin-right: 16px;" />
						</a>
						<a href="https://nsf.gov/" target="_blank">
							<img src="img/nsf.png" height="70" style="border-radius: 50%; margin-right: 16px;" />
						</a>
						<a href="http://npsc.org/" target="_blank">
							<img src="img/npsc.png" height="70" style="border-radius: 50%; margin-right: 16px;" />
						</a>
						<a href="https://www.hertzfoundation.org/" target="_blank">
							<img src="img/hertz.jpg" height="70" style="border-radius: 50%;" />
						</a>
					</div>
				</div>
			</div>
		</div>
    <!--
    <div>
      <div class="container py-5 text-center">
        <div class="row align-items-center">
          <div class="col-md-4 py-5 py-md-0">
            <img src="img/051-FIG-Motion.svg" class="w-100" />
          </div>
          <div class="col-md-8 ps-md-5 text-md-start">
            <h4 class="mb-3">Motion Features</h4>
            <p class="text-justify">Motion data (telemetry) is the primary source of data for user identification and inference in VR. Each frame of telemetry data encodes 3D position and orientation coordinates across each of the three tracked objects. Replacing the three Euler angles with four quaternion elements and summarizing each of these 21 data streams using five summary statistics, namely the minimum, maximum, mean, median, and standard deviation, results in a 105-dimensional motion feature vector.</p>
          </div>
        </div>
        <div class="row align-items-center mt-5">
          <div class="col-md-4 py-5 py-md-0 order-md-last">
            <img src="img/053-FIG-Context.svg" class="w-100" />
          </div>
          <div class="col-md-8 pe-md-5 text-md-start">
            <h4>Context Features</h4>
            <p class="text-justify">We found 22 features that most accurately characterize movement relative to a single event. These features include, for example, the position, orientation, type, and color of the object, the angle, speed, location, and accuracy of the motion, and the relative distance in both space and time. These context features and 105 motion features corresponding to the one-second intervals before and after the event, totalling 232 dimensions, can be used to identify users with a high degree of accuracy.</p>
          </div>
        </div>
        <div class="row align-items-center mt-5 mb-5">
          <div class="col-md-4 py-5 py-md-0">
            <img src="img/082-FIG-Features.svg" class="w-100" />
          </div>
          <div class="col-md-8 ps-md-5 text-md-start">
            <h4>Results</h4>
            <p class="text-justify">After training a classification model on 5 minutes of data per person, a user can be uniquely identified amongst the entire pool of 50,000+ with 94.33% accuracy from 100 seconds of motion, and with 73.20% accuracy from just 10 seconds of motion. Even with a single sample generated from just 2 seconds of telemetry data, the correct user out of 50,000 is identified about 48.45% of the time. Users with 5 or less total replays submitted were harder to identify, while users with 100 or more replays could be identified with over 99.5% accuracy. While static measurements comprise many of the most important features, they account for only 22.9% of the overall performance of the model. Motion features constitute 73.9% of all entropy gain, while contextual features compose the remaining 3.2%.</p>
          </div>
        </div>
      </div>
    </div>
  -->
    <div class="bg-dark text-white py-4">
      <div class="container text-center">
        <h5 class="mb-4"><i>Authors:</i></h5>
        <a style="display: inline-block" width="120px" href="https://nair.me">
          <img src="img/vivek.jpg" height="120" class="rounded-circle" />
          <p style="color: #fff; text-decoration: none; outline: none;">Vivek Nair</p>
        </a>
        <a style="display: inline-block" width="120px" href="https://henrygwb.github.io/">
          <img src="img/wenbo.jpg" height="120" class="rounded-circle" />
          <p style="color: #fff; text-decoration: none; outline: none;">Wenbo Guo</p>
        </a>
        <a style="display: inline-block" width="120px" href="https://github.com/Rui-Wang-813">
          <img src="img/rui.jfif" height="120" class="rounded-circle" />
          <p style="color: #fff; text-decoration: none; outline: none;">Rui Wang</p>
        </a>
        <a style="display: inline-block" width="120px" href="http://obrien.berkeley.edu/">
          <img src="img/james.jfif" height="120" class="rounded-circle" />
          <p style="color: #fff; text-decoration: none; outline: none;">James F. O'Brien</p>
        </a>
        <a style="display: inline-block" width="120px" href="https://dawnsong.io">
          <img src="img/dawn.jpg" height="120" class="rounded-circle" />
          <p style="color: #fff; text-decoration: none; outline: none;">Dawn Song</p>
        </a>
      </div>
    </div>
    <div class="bg-dark text-white text-center py-2">
      <div class="container">
        <p class="m-0">Copyright &copy;2023 UC Regents &nbsp;|&nbsp; Email us at <a href="mailto:rdi@berkeley.edu">rdi@berkeley.edu</a>.</p>
      </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-A3rJD856KowSb7dwlZdYEkO39Gagi7vIsF0jrRAoQmDKKtQBHUuLZ9AsSv4jD4Xa" crossorigin="anonymous"></script>
  </body>
</html>
